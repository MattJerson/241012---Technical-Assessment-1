{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6CJuwVUr57Z",
        "outputId": "67f83c0e-dd63-40a4-b4f7-0f8461fa1c3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded:\n",
            "                           user_input           intent    entities\n",
            "0           Book a flight to New York      book_flight    New York\n",
            "1  What's the weather like in London?      get_weather      London\n",
            "2                    Order me a pizza       order_food       pizza\n",
            "3            Find a restaurant nearby  find_restaurant  restaurant\n",
            "4                     Play some music       play_music         NaN\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You: Book a flight to New York\n",
            "Intent: get_weather\n",
            "Entities: [{'entity': 'I-LOC', 'score': 0.9995646, 'index': 5, 'word': 'New', 'start': 17, 'end': 20}, {'entity': 'I-LOC', 'score': 0.9992194, 'index': 6, 'word': 'York', 'start': 21, 'end': 25}]\n",
            "Please rate my response (1-5): 2\n",
            "I see that my responses for this topic could be improved.\n",
            "I'll try to give you better information next time!\n",
            "You: Book a flight to New York\n",
            "Intent: get_weather\n",
            "Entities: [{'entity': 'I-LOC', 'score': 0.9995646, 'index': 5, 'word': 'New', 'start': 17, 'end': 20}, {'entity': 'I-LOC', 'score': 0.9992194, 'index': 6, 'word': 'York', 'start': 21, 'end': 25}]\n",
            "Please rate my response (1-5): 5\n",
            "You: exit\n",
            "Goodbye!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, pipeline\n",
        "import nltk\n",
        "from collections import defaultdict\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv('conversational_data.csv')\n",
        "print(\"Dataset loaded:\")\n",
        "print(data.head())\n",
        "\n",
        "# Define intents\n",
        "intents = [\n",
        "    \"greeting\",\n",
        "    \"get_weather\",\n",
        "    \"set_alarm\",\n",
        "    \"book_flight\",\n",
        "    \"book_hotel\",\n",
        "    \"book_taxi\",\n",
        "    \"find_restaurant\",\n",
        "    \"get_news\",\n",
        "    \"schedule_meeting\",\n",
        "    \"tell_joke\",\n",
        "    \"play_music\",\n",
        "    \"make_reservation\",\n",
        "    \"get_traffic\",\n",
        "    \"get_showtimes\",\n",
        "    \"learn_language\",\n",
        "    \"order_food\",\n",
        "    \"set_reminder\",\n",
        "    \"get_quote\",\n",
        "    \"find_gym\"\n",
        "]\n",
        "\n",
        "num_intents = len(intents)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_intents)\n",
        "\n",
        "# Feedback storage\n",
        "feedback_storage = defaultdict(list)\n",
        "\n",
        "# Tokenization and fine-tuning (for demo purposes, this is a placeholder)\n",
        "def classify_intent(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "    outputs = model(**inputs)\n",
        "    probs = torch.softmax(outputs.logits, dim=1)\n",
        "    intent_label = torch.argmax(probs).item()\n",
        "    return intent_label\n",
        "\n",
        "# Use a pre-trained NER model for entity extraction\n",
        "ner_model = pipeline('ner', model=\"dbmdz/bert-large-cased-finetuned-conll03-english\", tokenizer=\"bert-large-cased\")\n",
        "\n",
        "def process_user_input(text):\n",
        "    intent_label = classify_intent(text)\n",
        "    intent = intents[intent_label]\n",
        "    entities = ner_model(text)\n",
        "    return intent, entities\n",
        "\n",
        "def format_output(intent, entities):\n",
        "    print(f\"Intent: {intent}\")\n",
        "    print(f\"Entities: {entities}\")\n",
        "\n",
        "# Feedback function\n",
        "def get_feedback():\n",
        "    while True:\n",
        "        try:\n",
        "            feedback = int(input(\"Please rate my response (1-5): \"))\n",
        "            if 1 <= feedback <= 5:\n",
        "                return feedback\n",
        "            else:\n",
        "                print(\"Please enter a valid number between 1 and 5.\")\n",
        "        except ValueError:\n",
        "            print(\"Invalid input. Please enter a number.\")\n",
        "\n",
        "def adjust_response(intent):\n",
        "    if feedback_storage[intent]:\n",
        "        average_feedback = sum(feedback_storage[intent]) / len(feedback_storage[intent])\n",
        "        if average_feedback <= 2:\n",
        "            print(\"I see that my responses for this topic could be improved.\")\n",
        "            print(\"I'll try to give you better information next time!\")\n",
        "    else:\n",
        "        print(\"Thanks for your feedback!\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() == \"exit\":\n",
        "        print(\"Goodbye!\")\n",
        "        break\n",
        "\n",
        "    intent, entities = process_user_input(user_input)\n",
        "\n",
        "    format_output(intent, entities)\n",
        "\n",
        "    feedback = get_feedback()\n",
        "    feedback_storage[intent].append(feedback)\n",
        "    adjust_response(intent)\n"
      ]
    }
  ]
}